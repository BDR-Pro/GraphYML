version: '3.8'

services:
  # Main GraphYML application
  graphyml:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./saved_yamls:/app/saved_yamls
      - ./graph_config.json:/app/graph_config.json
    environment:
      - EMBEDDING_SERVICE=http://embedding-service:8000/api/embeddings
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_HEADLESS=true
    depends_on:
      - embedding-service
    networks:
      - graphyml-network
    restart: unless-stopped

  # Embedding service using Hugging Face models
  embedding-service:
    build:
      context: .
      dockerfile: Dockerfile.embedding
    ports:
      - "8000:8000"
    volumes:
      - embedding-cache:/root/.cache/huggingface
    environment:
      - MODEL_NAME=all-MiniLM-L6-v2
      - PORT=8000
    networks:
      - graphyml-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # Optional Ollama service for local LLM embeddings
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - graphyml-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

networks:
  graphyml-network:
    driver: bridge

volumes:
  embedding-cache:
  ollama-models:

